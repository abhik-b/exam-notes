<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Artificial Intelligence (AI)</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Knowledge Space"/><meta property="og:title" content="Artificial Intelligence (AI)"/><meta property="og:description" content="Personal Knowledge Space"/><meta property="og:url" content="localhost:3000/notes/keojisysn3fz8lxhbdfcna4/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="21/9/2022"/><meta property="article:modified_time" content="21/9/2022"/><link rel="canonical" href="localhost:3000/notes/keojisysn3fz8lxhbdfcna4/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/16f9c67da7709054.css" as="style"/><link rel="stylesheet" href="/_next/static/css/16f9c67da7709054.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-750fad5a240459e2.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-c4b0e551a2150d17.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9ae633c69cbb1e91.js" defer=""></script><script src="/_next/static/chunks/855-124c7d93158bd007.js" defer=""></script><script src="/_next/static/chunks/776-959ab9b5289532fd.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-9c4f89da4e36492e.js" defer=""></script><script src="/_next/static/feL3KV3JtvScbJpYguJNn/_buildManifest.js" defer=""></script><script src="/_next/static/feL3KV3JtvScbJpYguJNn/_ssgManifest.js" defer=""></script><script src="/_next/static/feL3KV3JtvScbJpYguJNn/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div class="main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="artificial-intelligence-ai"><a aria-hidden="true" class="anchor-heading icon-link" href="#artificial-intelligence-ai"></a>Artificial Intelligence (AI)</h1>
<p>Machine Learning is a iterative process.</p>
<p><strong>Supervised Learning</strong> :
There is a direct relationship between input variable &#x26; output variable. Check the drawing below to understand more :
<img src="/assets/images/2022-09-21-14-39-10.png"></p>
<p><strong>Supervised learning</strong> problems can be classified into :</p>
<ul>
<li>
<p><em>regression problem</em> : Data itself is predicted. Example :House price prediction</p>
</li>
<li>
<p><em>classification problem</em> : Category is predicted by the data. Example : if we upload an image of an animal then whether that animal is cat or non cat .</p>
</li>
</ul>
<p><strong><em>Anomaly detection algorithm</em></strong> : Identify unusal data points. Example : Unusal traffic or strange patterns in network that can indicate network is hacked.</p>
<p><strong><em>Clustering algorithm</em></strong> : Groups data based on similiar conditions. Example : what types of customer buys a certain product</p>
<p><strong>Overfitting Data</strong> : When model tries to use all attributes (even the least important ones) due to excess knowledge of attributes training set score is increasing (or error is decreasing) while testing set score is decreasing (or error is increasing)</p>
<p><strong>Underfitting Data</strong> : model doesn't has enough knowledge and gives results which have very less connections/harmony among each other , due to lack of enough attributes.</p>
<h2 id="linear-algebra--regression--classification"><a aria-hidden="true" class="anchor-heading icon-link" href="#linear-algebra--regression--classification"></a>Linear Algebra , Regression &#x26; Classification</h2>
<p>Scalar,Vector,Matrix,Matrix Operations,Norms,Probaility,Joint Distribution,Bayes theorem , Expectation, COvariance.</p>
<p>Vector dot product = each row * each column</p>
<h2 id="linear-regression"><a aria-hidden="true" class="anchor-heading icon-link" href="#linear-regression"></a>Linear Regression</h2>
<hr>
<p>Read this to learn more about <a href="https://realpython.com/python-virtual-environments-a-primer/#why-do-you-need-virtual-environments">Why Virtual Environments</a></p>
<h3 id="what-is-pytorch-tensor-"><a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-pytorch-tensor-"></a>What is PyTorch Tensor ??</h3>
<p>A Pytorch Tensor is just a generic <strong>n-dimensional array</strong> to be used for arbitrary numeric computation. It can run on either CPU or GPU.
Read this article to learn more : <a href="https://www.geeksforgeeks.org/tensors-in-pytorch/">Tensors in PyTorch</a></p>
<h3 id="batch-sizes"><a aria-hidden="true" class="anchor-heading icon-link" href="#batch-sizes"></a>Batch Sizes</h3>
<p>For example I have 1000 samples of data for training &#x26; I set batch_size = 10 then first 10 data samples (i.e from 1 to 10 ) will be used to train. Then again next 10 data samples (i.e from 11 to 20) will be used to train. Simliarly all 100 datat samples will be trained by training 10 samples at once.
Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost). Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.</p>
<h3 id="in-the-neural-network-terminology"><a aria-hidden="true" class="anchor-heading icon-link" href="#in-the-neural-network-terminology"></a>In the neural network terminology:</h3>
<ul>
<li><em>one epoch</em> = one forward pass and one backward pass of all the training examples</li>
<li><em>batch size</em> = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.</li>
<li><em>number of iterations</em> = number of passes, each pass using batch size as number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).</li>
</ul>
<p>Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.</p>
<h3 id="shuffles"><a aria-hidden="true" class="anchor-heading icon-link" href="#shuffles"></a>Shuffles</h3>
<p>For Example ,We want our neural net to recognize each number from the hand drawings &#x26; we are using a dataset that has all handdrawn numbers form 0 to 9 to train the net. Now if we took a neural net &#x26; fed through a bunch of 0s 1st then the neural net will say all are 0s , then when we fed 1s then the neural net will say all are 1s &#x26; when we reach 9s then it will end saying all are 9s. Now it will only recognize 9s but not the previous numbers but we want the net to recognize each number. Shuffling will make sure it learns all the numbers by trying to learn all numbers together.</p>
<h3 id="layers-hand-drawn-numbers-training"><a aria-hidden="true" class="anchor-heading icon-link" href="#layers-hand-drawn-numbers-training"></a>Layers (Hand Drawn Numbers training)</h3>
<p>We are trying to have 3 hidden layers of 64 neurons. First layer will have Input of <code>28*28</code> (image size) &#x26; out put will be 64.Then next 2 layers will have 64 as both input &#x26; output sizes. Then the last layer will have input of 64 but output will be 10 . The 2nd , 3rd ,4th layer's input is 64 because the prev layer outputs 64 &#x26; this 64 then gets inputted to this layer.The 4th layer outputs 10 beacuse hand drawn numbers have 0 to 9 numbers (10 numbers in total) so 10 classes will be outputted.</p>
<p><img src="/assets/images/2022-09-21-14-41-30.png"></p>
<h3 id="feed-forward"><a aria-hidden="true" class="anchor-heading icon-link" href="#feed-forward"></a>Feed Forward</h3>
<p>In the prev section we created some layers now we need a path to take data from each layer to another. A feedforward neural network (FNN) is an artificial neural network wherein connections between the nodes do not form a cycle. The feedforward neural network was the first and simplest type of artificial neural network devised.</p>
<p><img src="/assets/images/2022-09-21-14-41-51.png"></p>
<p><img src="/assets/images/2022-09-21-14-42-07.png"></p>
<p>X will be passed to each layer however our model wont learn much because data wont be scaled properly. Scaled properly means data is transformed such that the features are within a specific range e.g. [0, 1]. For proper data scaling we need <a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">activation functions</a>. So here we will use ReLU activation. Activation Functions work on output of layers. So for the 1st 3 layers we will use ReLU Activation Functions but for the 4th layer we will use something else.
4th layer has 10 neurons (0-9 numbers) so we want to optimize the output so that 1 neuron is fully fired than the other 9. For that we will use softmax.</p>
<p><img src="/assets/images/2022-09-21-14-43-00.png"></p>
<p>Notice that in the above image I have a forward method. This forward is called when Net Model is called (in line : <code>output=net(X)</code> ) . When you call the model directly this Forward is called , the internal  <code>__call__</code>  function is used. This function manages all registered hooks and calls forward afterwards. That’s also the reason you should call the model directly, because otherwise your hooks might not work etc.</p>
<h3 id="training-model"><a aria-hidden="true" class="anchor-heading icon-link" href="#training-model"></a>Training Model</h3>
<p>Loss &#x26; optimizer are needed to make sure the model is nearly accurate. Loss means how wrong the model is. Optimizer makes sure the loss is reduced slowly over time based on the learning rate.</p>
<p>When we pass data through the neural network , it incurs a loss. Now it is entirely calculable to get weights to make loss zero. If we do that then our model might. So we use learning rate to opimize to lower the loss but only take certain size steps to opimize(not gigantic steps because then the optimization might stuck).</p>
<p><img src="/assets/images/2022-09-21-14-43-36.png"></p>
<h3 id="neural-net-from-scratch"><a aria-hidden="true" class="anchor-heading icon-link" href="#neural-net-from-scratch"></a>Neural net from scratch</h3>
<p>Let A0 be the input layer, Z1 &#x26; Z2 be the unactivated hidden layer. We will get the dot product of W1 and A0. W1 is bunch of weights that correspond to each of the 7840 connections. b1 is a constant bias term added to each of the 10 nodes. Then we will add activation functions to Z1.
Without applying activation function each node in the 1st layer would be <em>a linear combination of nodes before it + bias term</em> . Then the second layer will be a linear combo of 1st layer &#x26; finally the output layer will be linear combo of 2nd layer which is a linear combo of 1st layer which is linear combo of input layer. This is just a fancy linear regression &#x26; will not become a smart learner. For more info <a href="https://qr.ae/pvOVWB">read this</a></p>
<p><img src="/assets/images/2022-09-21-14-44-02.png"></p>
<p>We will use Rectified Linear Unit. A1 is the ==output== of ReLU applied on Z1. Then in similiar fashion we will get Z2 which is the second layer of hidden layers. However we will use Softmax activation function on Z2 to get the output layer (A2) .</p>
<p><img src="/assets/images/2022-09-21-14-44-34.png"></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#linear-algebra--regression--classification" title="Linear Algebra , Regression &amp; Classification">Linear Algebra , Regression &amp; Classification</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#linear-regression" title="Linear Regression">Linear Regression</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#what-is-pytorch-tensor-" title="What is PyTorch Tensor ??">What is PyTorch Tensor ??</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#batch-sizes" title="Batch Sizes">Batch Sizes</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#in-the-neural-network-terminology" title="In the neural network terminology:">In the neural network terminology:</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#shuffles" title="Shuffles">Shuffles</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#layers-hand-drawn-numbers-training" title="Layers (Hand Drawn Numbers training)">Layers (Hand Drawn Numbers training)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#feed-forward" title="Feed Forward">Feed Forward</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#training-model" title="Training Model">Training Model</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#neural-net-from-scratch" title="Neural net from scratch">Neural net from scratch</a></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"keojisysn3fz8lxhbdfcna4","title":"Artificial Intelligence (AI)","desc":"","updated":1663751768488,"created":1663751303455,"custom":{},"fname":"ai","type":"note","vault":{"fsPath":".","selfContained":true,"name":"exam-notes"},"contentHash":"3ad06c758c1fa47a97e9ba5e77b3907d","links":[],"anchors":{"linear-algebra--regression--classification":{"type":"header","text":"Linear Algebra , Regression \u0026 Classification","value":"linear-algebra--regression--classification","line":28,"column":0,"depth":2},"linear-regression":{"type":"header","text":"Linear Regression","value":"linear-regression","line":34,"column":0,"depth":2},"what-is-pytorch-tensor-":{"type":"header","text":"What is PyTorch Tensor ??","value":"what-is-pytorch-tensor-","line":40,"column":0,"depth":3},"batch-sizes":{"type":"header","text":"Batch Sizes","value":"batch-sizes","line":45,"column":0,"depth":3},"in-the-neural-network-terminology":{"type":"header","text":"In the neural network terminology:","value":"in-the-neural-network-terminology","line":50,"column":0,"depth":3},"shuffles":{"type":"header","text":"Shuffles","value":"shuffles","line":58,"column":0,"depth":3},"layers-hand-drawn-numbers-training":{"type":"header","text":"Layers (Hand Drawn Numbers training)","value":"layers-hand-drawn-numbers-training","line":62,"column":0,"depth":3},"feed-forward":{"type":"header","text":"Feed Forward","value":"feed-forward","line":68,"column":0,"depth":3},"training-model":{"type":"header","text":"Training Model","value":"training-model","line":83,"column":0,"depth":3},"neural-net-from-scratch":{"type":"header","text":"Neural net from scratch","value":"neural-net-from-scratch","line":91,"column":0,"depth":3}},"children":[],"parent":"gjr379x4uby1klukcwuuafc","data":{}},"body":"\u003ch1 id=\"artificial-intelligence-ai\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#artificial-intelligence-ai\"\u003e\u003c/a\u003eArtificial Intelligence (AI)\u003c/h1\u003e\n\u003cp\u003eMachine Learning is a iterative process.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e :\nThere is a direct relationship between input variable \u0026#x26; output variable. Check the drawing below to understand more :\n\u003cimg src=\"/assets/images/2022-09-21-14-39-10.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSupervised learning\u003c/strong\u003e problems can be classified into :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eregression problem\u003c/em\u003e : Data itself is predicted. Example :House price prediction\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eclassification problem\u003c/em\u003e : Category is predicted by the data. Example : if we upload an image of an animal then whether that animal is cat or non cat .\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAnomaly detection algorithm\u003c/em\u003e\u003c/strong\u003e : Identify unusal data points. Example : Unusal traffic or strange patterns in network that can indicate network is hacked.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eClustering algorithm\u003c/em\u003e\u003c/strong\u003e : Groups data based on similiar conditions. Example : what types of customer buys a certain product\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOverfitting Data\u003c/strong\u003e : When model tries to use all attributes (even the least important ones) due to excess knowledge of attributes training set score is increasing (or error is decreasing) while testing set score is decreasing (or error is increasing)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderfitting Data\u003c/strong\u003e : model doesn't has enough knowledge and gives results which have very less connections/harmony among each other , due to lack of enough attributes.\u003c/p\u003e\n\u003ch2 id=\"linear-algebra--regression--classification\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#linear-algebra--regression--classification\"\u003e\u003c/a\u003eLinear Algebra , Regression \u0026#x26; Classification\u003c/h2\u003e\n\u003cp\u003eScalar,Vector,Matrix,Matrix Operations,Norms,Probaility,Joint Distribution,Bayes theorem , Expectation, COvariance.\u003c/p\u003e\n\u003cp\u003eVector dot product = each row * each column\u003c/p\u003e\n\u003ch2 id=\"linear-regression\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#linear-regression\"\u003e\u003c/a\u003eLinear Regression\u003c/h2\u003e\n\u003chr\u003e\n\u003cp\u003eRead this to learn more about \u003ca href=\"https://realpython.com/python-virtual-environments-a-primer/#why-do-you-need-virtual-environments\"\u003eWhy Virtual Environments\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"what-is-pytorch-tensor-\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-is-pytorch-tensor-\"\u003e\u003c/a\u003eWhat is PyTorch Tensor ??\u003c/h3\u003e\n\u003cp\u003eA Pytorch Tensor is just a generic \u003cstrong\u003en-dimensional array\u003c/strong\u003e to be used for arbitrary numeric computation. It can run on either CPU or GPU.\nRead this article to learn more : \u003ca href=\"https://www.geeksforgeeks.org/tensors-in-pytorch/\"\u003eTensors in PyTorch\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"batch-sizes\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#batch-sizes\"\u003e\u003c/a\u003eBatch Sizes\u003c/h3\u003e\n\u003cp\u003eFor example I have 1000 samples of data for training \u0026#x26; I set batch_size = 10 then first 10 data samples (i.e from 1 to 10 ) will be used to train. Then again next 10 data samples (i.e from 11 to 20) will be used to train. Simliarly all 100 datat samples will be trained by training 10 samples at once.\nGradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost). Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.\u003c/p\u003e\n\u003ch3 id=\"in-the-neural-network-terminology\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#in-the-neural-network-terminology\"\u003e\u003c/a\u003eIn the neural network terminology:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eone epoch\u003c/em\u003e = one forward pass and one backward pass of all the training examples\u003c/li\u003e\n\u003cli\u003e\u003cem\u003ebatch size\u003c/em\u003e = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003enumber of iterations\u003c/em\u003e = number of passes, each pass using batch size as number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\u003c/p\u003e\n\u003ch3 id=\"shuffles\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#shuffles\"\u003e\u003c/a\u003eShuffles\u003c/h3\u003e\n\u003cp\u003eFor Example ,We want our neural net to recognize each number from the hand drawings \u0026#x26; we are using a dataset that has all handdrawn numbers form 0 to 9 to train the net. Now if we took a neural net \u0026#x26; fed through a bunch of 0s 1st then the neural net will say all are 0s , then when we fed 1s then the neural net will say all are 1s \u0026#x26; when we reach 9s then it will end saying all are 9s. Now it will only recognize 9s but not the previous numbers but we want the net to recognize each number. Shuffling will make sure it learns all the numbers by trying to learn all numbers together.\u003c/p\u003e\n\u003ch3 id=\"layers-hand-drawn-numbers-training\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#layers-hand-drawn-numbers-training\"\u003e\u003c/a\u003eLayers (Hand Drawn Numbers training)\u003c/h3\u003e\n\u003cp\u003eWe are trying to have 3 hidden layers of 64 neurons. First layer will have Input of \u003ccode\u003e28*28\u003c/code\u003e (image size) \u0026#x26; out put will be 64.Then next 2 layers will have 64 as both input \u0026#x26; output sizes. Then the last layer will have input of 64 but output will be 10 . The 2nd , 3rd ,4th layer's input is 64 because the prev layer outputs 64 \u0026#x26; this 64 then gets inputted to this layer.The 4th layer outputs 10 beacuse hand drawn numbers have 0 to 9 numbers (10 numbers in total) so 10 classes will be outputted.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-41-30.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"feed-forward\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#feed-forward\"\u003e\u003c/a\u003eFeed Forward\u003c/h3\u003e\n\u003cp\u003eIn the prev section we created some layers now we need a path to take data from each layer to another. A feedforward neural network (FNN) is an artificial neural network wherein connections between the nodes do not form a cycle. The feedforward neural network was the first and simplest type of artificial neural network devised.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-41-51.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-42-07.png\"\u003e\u003c/p\u003e\n\u003cp\u003eX will be passed to each layer however our model wont learn much because data wont be scaled properly. Scaled properly means data is transformed such that the features are within a specific range e.g. [0, 1]. For proper data scaling we need \u003ca href=\"https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\"\u003eactivation functions\u003c/a\u003e. So here we will use ReLU activation. Activation Functions work on output of layers. So for the 1st 3 layers we will use ReLU Activation Functions but for the 4th layer we will use something else.\n4th layer has 10 neurons (0-9 numbers) so we want to optimize the output so that 1 neuron is fully fired than the other 9. For that we will use softmax.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-43-00.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNotice that in the above image I have a forward method. This forward is called when Net Model is called (in line : \u003ccode\u003eoutput=net(X)\u003c/code\u003e ) . When you call the model directly this Forward is called , the internal  \u003ccode\u003e__call__\u003c/code\u003e  function is used. This function manages all registered hooks and calls forward afterwards. That’s also the reason you should call the model directly, because otherwise your hooks might not work etc.\u003c/p\u003e\n\u003ch3 id=\"training-model\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#training-model\"\u003e\u003c/a\u003eTraining Model\u003c/h3\u003e\n\u003cp\u003eLoss \u0026#x26; optimizer are needed to make sure the model is nearly accurate. Loss means how wrong the model is. Optimizer makes sure the loss is reduced slowly over time based on the learning rate.\u003c/p\u003e\n\u003cp\u003eWhen we pass data through the neural network , it incurs a loss. Now it is entirely calculable to get weights to make loss zero. If we do that then our model might. So we use learning rate to opimize to lower the loss but only take certain size steps to opimize(not gigantic steps because then the optimization might stuck).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-43-36.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"neural-net-from-scratch\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#neural-net-from-scratch\"\u003e\u003c/a\u003eNeural net from scratch\u003c/h3\u003e\n\u003cp\u003eLet A0 be the input layer, Z1 \u0026#x26; Z2 be the unactivated hidden layer. We will get the dot product of W1 and A0. W1 is bunch of weights that correspond to each of the 7840 connections. b1 is a constant bias term added to each of the 10 nodes. Then we will add activation functions to Z1.\nWithout applying activation function each node in the 1st layer would be \u003cem\u003ea linear combination of nodes before it + bias term\u003c/em\u003e . Then the second layer will be a linear combo of 1st layer \u0026#x26; finally the output layer will be linear combo of 2nd layer which is a linear combo of 1st layer which is linear combo of input layer. This is just a fancy linear regression \u0026#x26; will not become a smart learner. For more info \u003ca href=\"https://qr.ae/pvOVWB\"\u003eread this\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-44-02.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWe will use Rectified Linear Unit. A1 is the ==output== of ReLU applied on Z1. Then in similiar fashion we will get Z2 which is the second layer of hidden layers. However we will use Softmax activation function on Z2 to get the output layer (A2) .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2022-09-21-14-44-34.png\"\u003e\u003c/p\u003e","noteIndex":{"id":"gjr379x4uby1klukcwuuafc","title":"Root","desc":"","updated":1663749110634,"created":1663740327223,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"exam-notes"},"contentHash":"f0577dcf4f8c635682dea6fed1600903","links":[],"anchors":{"notes-taken-by-abhik-b-for-competitive-exams":{"type":"header","text":"Notes taken by Abhik B for competitive exams","value":"notes-taken-by-abhik-b-for-competitive-exams","line":8,"column":0,"depth":1}},"children":["keojisysn3fz8lxhbdfcna4","nx2wuyeo5kgob0u4jucsers","gx0wes5gz0irclfjvffusa4"],"parent":null,"data":{},"body":"\n# Notes taken by Abhik B for competitive exams\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"exam-notes"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"keojisysn3fz8lxhbdfcna4"},"buildId":"feL3KV3JtvScbJpYguJNn","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>